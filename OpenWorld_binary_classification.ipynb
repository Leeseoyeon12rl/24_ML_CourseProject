{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datafile...\n",
      "Total closed world dataset samples: 19000\n"
     ]
    }
   ],
   "source": [
    "# Load Closed World dataset\n",
    "\n",
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS   = 950\n",
    "\n",
    "# Load the pickle file\n",
    "print(\"Loading datafile...\")\n",
    "with open('mon_standard.pkl', 'rb') as fi: # mon_standard.pkl in directory\n",
    "    data = pickle.load(fi)\n",
    "\n",
    "X1_closed = [] # Array to store instances (timestamps) - 19,000 instances\n",
    "X2_closed = [] # Array to store instances (direction*size) - size information\n",
    "y_closed = [] # Array to store the site of each instance - 19,000 instances\n",
    "\n",
    "# Differentiate instances and sites, and store them in the respective x and y arrays\n",
    "# x array (direction*timestamp), y array (site label)\n",
    "\n",
    "for i in range(TOTAL_URLS):\n",
    "    for sample in data[i]:\n",
    "        size_seq = []\n",
    "        time_seq = []\n",
    "        for c in sample:\n",
    "            dr = 1 if c > 0 else -1\n",
    "            time_seq.append(abs(c))\n",
    "            size_seq.append(dr * 512)\n",
    "        X1_closed.append(time_seq)\n",
    "        X2_closed.append(size_seq)\n",
    "        y_closed.append(1)  # 모든 레이블을 1로 설정\n",
    "\n",
    "print(f'Total closed world dataset samples: {len(y_closed)}')\n",
    "\n",
    "# 새로운 pkl 파일의 구성\n",
    "output_data = {\n",
    "    'X1_closed': X1_closed,\n",
    "    'X2_closed': X2_closed,\n",
    "    'y_closed': y_closed\n",
    "}\n",
    "\n",
    "# 파일 저장\n",
    "output_file = 'mon_standard_withlabel_binary.pkl'\n",
    "\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 로드\n",
    "with open('mon_standard_withlabel_binary.pkl', 'rb') as f:\n",
    "    monitored_data = pickle.load(f)\n",
    "\n",
    "with open('unmon_standard10_3000_withlabel.pkl', 'rb') as f: # already downloaded by debugging OpenWorld_multiclass_classification.ipyng\n",
    "    unmonitored_data = pickle.load(f)\n",
    "\n",
    "# Monitored 데이터\n",
    "monitored_X1 = monitored_data['X1_closed']\n",
    "monitored_X2 = monitored_data['X2_closed']\n",
    "monitored_y = monitored_data['y_closed']  # 레이블은 '1'로 지정됨\n",
    "\n",
    "# Unmonitored 데이터\n",
    "unmonitored_X1 = unmonitored_data['X1_open']\n",
    "unmonitored_X2 = unmonitored_data['X2_open']\n",
    "unmonitored_y = unmonitored_data['y_open']  # 레이블은 `-1`로 지정됨\n",
    "\n",
    "# 데이터 병합\n",
    "X1 = monitored_X1 + unmonitored_X1\n",
    "X2 = monitored_X2 + unmonitored_X2\n",
    "y = monitored_y + unmonitored_y\n",
    "\n",
    "# 병합된 데이터 구조 생성\n",
    "combined_data = {\n",
    "    'X1': X1,\n",
    "    'X2': X2,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "# 파일로 저장\n",
    "output_file = 'combined_dataset_binary.pkl'\n",
    "\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(combined_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined dataset\n",
    "with open('combined_dataset_binary.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "# Feature extraction\n",
    "pkt_sizes = X2\n",
    "pkt_timestamps = X1\n",
    "\n",
    "# Extract cumulative packet sizes\n",
    "cumulative_pkt_sizes = [np.cumsum(packet_sizes).tolist() for packet_sizes in pkt_sizes]\n",
    "X3 = cumulative_pkt_sizes\n",
    "\n",
    "# Extract bursts\n",
    "bursts = []\n",
    "for instance in pkt_sizes:\n",
    "    instance_bursts = []\n",
    "    current_burst = 0\n",
    "    current_direction = np.sign(instance[0])  # Initialize with the first packet's direction\n",
    "    for size in instance:\n",
    "        direction = np.sign(size)\n",
    "        if direction == current_direction:\n",
    "            # Accumulate burst size for the same direction\n",
    "            current_burst += size\n",
    "        else:\n",
    "            # Append the completed burst and reset for the new direction\n",
    "            instance_bursts.append(current_burst)\n",
    "            current_burst = size\n",
    "            current_direction = direction\n",
    "    instance_bursts.append(current_burst)  # Add the last burst\n",
    "    bursts.append(instance_bursts)\n",
    "X4 = bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 22000\n"
     ]
    }
   ],
   "source": [
    "print(f'Total samples: {len(y)}') # monitored 19,000 + unmonitored 3,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X1, X2, X3, X4 into a single feature list X\n",
    "X = []\n",
    "\n",
    "for i in range(len(X1)):\n",
    "    instance_features = {\n",
    "        'timestamps': X1[i],\n",
    "        'packet_sizes': X2[i],\n",
    "        'cumulative_pkt_sizes': X3[i],\n",
    "        'bursts': X4[i]\n",
    "    }\n",
    "    X.append(instance_features)\n",
    "\n",
    "\n",
    "# Preprocessing X for Machine Learning\n",
    "# Step 1: Flatten each feature vector for each instance in X\n",
    "X_flattened = []\n",
    "\n",
    "for instance in X:\n",
    "    # Flatten each instance to a single feature vector (timestamps, packet_sizes, cumulative_pkt_sizes, bursts)\n",
    "    feature_vector = (\n",
    "        instance['timestamps'] +\n",
    "        instance['packet_sizes'] +\n",
    "        instance['cumulative_pkt_sizes'] +\n",
    "        instance['bursts']\n",
    "    )\n",
    "    X_flattened.append(feature_vector)\n",
    "\n",
    "# Perform train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flattened, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Pad sequences with -1 using numpy to ensure all feature vectors are the same length\n",
    "max_length = 10000  # Define maximum length to which sequences will be padded\n",
    "\n",
    "# Pad each feature vector to the maximum length with -1\n",
    "X_train_padded = np.array([\n",
    "    np.pad(fv, (0, max(0, max_length - len(fv))), mode='constant', constant_values=-1)[:max_length]\n",
    "    for fv in X_train\n",
    "])\n",
    "\n",
    "# 동일한 방식으로 X_test에도 적용\n",
    "X_test_padded = np.array([\n",
    "    np.pad(fv, (0, max(0, max_length - len(fv))), mode='constant', constant_values=-1)[:max_length]\n",
    "    for fv in X_test\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_padded = scaler.fit_transform(X_train_padded)\n",
    "X_test_padded = scaler.transform(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 정확도: 0.9755\n"
     ]
    }
   ],
   "source": [
    "# Binary Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Logistic Regression 모델 정의\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)  # max_iter = 1000\n",
    "\n",
    "# 모델 학습\n",
    "log_reg.fit(X_train_padded, y_train)\n",
    "\n",
    "# 테스트 세트로 예측\n",
    "y_pred_log_reg = log_reg.predict(X_test_padded)\n",
    "\n",
    "# 정확도 평가\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "print(f\"Logistic Regression 정확도: {log_reg_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
